{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c72b440-eafd-4adf-921f-130c86ee54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fc644-eb14-468b-90ec-6557170c90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/mini_speech_commands'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face38cf-e977-4829-8f82-b3f161fb255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=16000,\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4169b-d41d-4f9c-845b-829171d19fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "def make_spec_ds(ds):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "class ExportModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "    # Accept either a string-filename or a batch of waveforms.\n",
    "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "    self.__call__.get_concrete_function(\n",
    "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    # If they pass a string, load the file and decode it. \n",
    "    if x.dtype == tf.string:\n",
    "      x = tf.io.read_file(x)\n",
    "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "      x = tf.squeeze(x, axis=-1)\n",
    "      x = x[tf.newaxis, :]\n",
    "\n",
    "    x = get_spectrogram(x)  \n",
    "    result = self.model(x, training=False)\n",
    "\n",
    "    class_ids = tf.argmax(result, axis=-1)\n",
    "    class_names = tf.gather(label_names, class_ids)\n",
    "    return {'predictions':result,\n",
    "            'class_ids': class_ids,\n",
    "            'class_names': class_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea36a1-3874-40c6-979f-90dc1ae7350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb78938",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)\n",
    "\n",
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = example_spectrograms.shape[1:]\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19239029",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_spectrogram_ds,\n",
    "    validation_data=val_spectrogram_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b393df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
    "x = tf.io.read_file(str(x))\n",
    "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "x = tf.squeeze(x, axis=-1)\n",
    "waveform = x\n",
    "x = get_spectrogram(x)\n",
    "x = x[tf.newaxis,...]\n",
    "\n",
    "prediction = model(x)\n",
    "plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "plt.title('No')\n",
    "plt.show()\n",
    "\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = ExportModel(model)\n",
    "export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(export, \"saved\")\n",
    "imported = tf.saved_model.load(\"saved\")\n",
    "imported(waveform[tf.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download LJSPeech Data set from https://www.kaggle.com/datasets/mathurinache/the-lj-speech-dataset/code\n",
    "#Following tutorial here https://www.youtube.com/watch?v=qKz_lmgad3o for the implementation of DeepSpeech like model\n",
    "#https://arxiv.org/pdf/1412.5567.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfe1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/LJSpeech-1.1'\n",
    "WAV_PATH = DATASET_PATH + '/wavs/'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH + \"/metadata.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_dir, sep=\"|\", header=None, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df807ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>normalized_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ031-0079</td>\n",
       "      <td>He made the official determination because the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ035-0144</td>\n",
       "      <td>Oswald's departure from building.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ009-0140</td>\n",
       "      <td>no inconvenience was suffered by the congregat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                              normalized_transcript\n",
       "0  LJ031-0079  He made the official determination because the...\n",
       "1  LJ035-0144                  Oswald's departure from building.\n",
       "2  LJ009-0140  no inconvenience was suffered by the congregat..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\"id\", \"transcript\", \"normalized_transcript\"]\n",
    "df = df[[\"id\", \"normalized_transcript\"]]\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147d852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'a', 'b', 'c', 'e', 'd', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', ' '](size=31)\n"
     ]
    }
   ],
   "source": [
    "characters = [x for x in \"abcedfghijklmnopqrstuvwxyz'?! \"]\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "num_to_char = keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)\n",
    "\n",
    "print(\n",
    "f\"The vocabulary is: {char_to_num.get_vocabulary()}\"\n",
    "f\"(size={char_to_num.vocabulary_size()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbef569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 11135\n",
      "Size of test set: 1965\n"
     ]
    }
   ],
   "source": [
    "split = int(len(df) * 0.85)\n",
    "df_train = df[:split]\n",
    "df_test = df[split:]\n",
    "\n",
    "print(f\"Size of training set: {len(df_train)}\")\n",
    "print(f\"Size of test set: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016901d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 256\n",
    "frame_step = 160\n",
    "fft_length = 384\n",
    "\n",
    "def encode_single_sample(wav, label):\n",
    "    file = tf.io.read_file(WAV_PATH + wav + \".wav\")\n",
    "    \n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    audio =tf.cast(audio, tf.float32) #Look up why float32?\n",
    "    spectrogram = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    \n",
    "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    \n",
    "    label = tf.strings.lower(label)\n",
    "    \n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    label = char_to_num(label)\n",
    "    \n",
    "    return spectrogram, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52084cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_train[\"id\"]), list(df_train[\"normalized_transcript\"]))\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_test[\"id\"]), list(df_test[\"normalized_transcript\"]))\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    test_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c69130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "    \n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    \n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7486f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
    "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
    "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
    "    \n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 41],\n",
    "        strides=[2, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_1\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 21],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    \n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "    \n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirection_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "            \n",
    "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
    "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    \n",
    "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
    "    model = keras.Model(input_spectrogram, output, name=\"deepSpeech_2\")\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=CTCLoss)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb0944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 193)]                         0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 193, 1)                        0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14432            \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 1568)                          0                \n",
      "                                                                                                              \n",
      " bidirection_1 (Bidirectional)                   (None, None, 1024)                          6395904          \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirection_2 (Bidirectional)                   (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_1 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirection_3 (Bidirectional)                   (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_2 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirection_4 (Bidirectional)                   (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_3 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirection_5 (Bidirectional)                   (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 1024)                          1049600          \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dropout_4 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 32)                            32800            \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 26,628,480\n",
      "Trainable params: 26,628,352\n",
      "Non-trainable params: 128\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=512,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e129769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A unitlity function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    \n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "    \n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result))/numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "    return output_text\n",
    "\n",
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model.predict(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = (\n",
    "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "                )\n",
    "                targets.append(label)\n",
    "            wer_score = wer(targets, predictions)\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            for i in np.random.randint(0, len(prediction), 5):\n",
    "                print(f\"Target\\t: {targets[i]}\")\n",
    "                print(f\"Prediction\\t: {predictions[i]}\")\n",
    "                print(\"-\" * 100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e7c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/LJSpeech-1.1/wavs/\n"
     ]
    }
   ],
   "source": [
    "print(WAV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75721e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node gradient_tape/CTCLoss/Shape/_92}}]] [Op:__inference_train_function_27333]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      2\u001b[0m test_callback \u001b[38;5;241m=\u001b[39m CallbackEval(test_dataset)\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ASR473Project\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ASR473Project\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mCancelledError\u001b[0m: Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node gradient_tape/CTCLoss/Shape/_92}}]] [Op:__inference_train_function_27333]"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "test_callback = CallbackEval(test_dataset)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[test_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "targets = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    X, y = batch\n",
    "    batch_predictions = model.predict(X)\n",
    "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "    predictions.extend(batch_predictions)\n",
    "    for label in y:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        targets.append(label)\n",
    "    wer_score = wer(targets, predictions)\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "    print(\"-\" * 100)\n",
    "    for i in np.random.randint(0, len(prediction), 5):\n",
    "        print(f\"Target\\t: {targets[i]}\")\n",
    "        print(f\"Prediction\\t: {predictions[i]}\")\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
